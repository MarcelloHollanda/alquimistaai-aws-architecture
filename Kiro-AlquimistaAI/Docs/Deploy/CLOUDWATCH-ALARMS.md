# CloudWatch Alarms - Fibonacci Ecosystem

## Overview

Este documento descreve os alarmes do CloudWatch configurados para monitorar a sa√∫de e performance do Ecossistema Alquimista.AI. Os alarmes s√£o essenciais para detectar problemas antes que afetem os usu√°rios e garantir SLA adequado.

## Arquitetura de Notifica√ß√µes

```
CloudWatch Alarms
       ‚Üì
   SNS Topic (fibonacci-alarms-{env})
       ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Email Subscriptions       ‚îÇ
   ‚îÇ  Slack Webhooks (futuro)   ‚îÇ
   ‚îÇ  PagerDuty (futuro)        ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Alarmes Configurados

### 1. High Error Rate Alarm

**Nome**: `fibonacci-high-error-rate-{env}`

**Descri√ß√£o**: Detecta taxa de erro alta na Lambda principal

**M√©trica**: Lambda Errors (Sum)

**Threshold**: ‚â• 10 erros em 2 minutos consecutivos

**A√ß√£o Recomendada**:
1. Verificar logs do CloudWatch: `/aws/lambda/fibonacci-api-handler-{env}`
2. Verificar X-Ray traces para identificar causa raiz
3. Verificar se h√° mudan√ßas recentes no c√≥digo
4. Verificar integra√ß√µes externas (MCP servers)

**Severidade**: üî¥ CR√çTICA

---

### 2. High Latency Alarm

**Nome**: `fibonacci-high-latency-{env}`

**Descri√ß√£o**: Detecta lat√™ncia alta (P95) na Lambda principal

**M√©trica**: Lambda Duration (p95)

**Threshold**: ‚â• 3000ms (3 segundos) por 2 de 3 per√≠odos de 5 minutos

**A√ß√£o Recomendada**:
1. Verificar X-Ray traces para identificar gargalos
2. Verificar performance do Aurora (CPU, connections)
3. Verificar lat√™ncia de integra√ß√µes MCP
4. Considerar aumentar mem√≥ria da Lambda
5. Verificar cold starts (adicionar provisioned concurrency se necess√°rio)

**Severidade**: üü° ALTA

---

### 3. DLQ Not Empty Alarm

**Nome**: `fibonacci-dlq-not-empty-{env}`

**Descri√ß√£o**: Detecta mensagens na Dead Letter Queue

**M√©trica**: SQS ApproximateNumberOfMessagesVisible (Maximum)

**Threshold**: ‚â• 1 mensagem

**A√ß√£o Recomendada**:
1. Acessar console SQS e inspecionar mensagens na DLQ
2. Identificar padr√£o de falhas (mesmo lead_id, mesmo agente, etc)
3. Verificar logs do agente que falhou
4. Corrigir problema e reprocessar mensagens manualmente
5. Purgar DLQ ap√≥s corre√ß√£o

**Severidade**: üî¥ CR√çTICA

**Script de Reprocessamento**:
```bash
# Mover mensagens da DLQ de volta para fila principal
aws sqs receive-message \
  --queue-url https://sqs.us-east-1.amazonaws.com/{account}/fibonacci-dlq-{env} \
  --max-number-of-messages 10 \
  --output json | \
jq -r '.Messages[] | .Body' | \
while read body; do
  aws sqs send-message \
    --queue-url https://sqs.us-east-1.amazonaws.com/{account}/fibonacci-main-{env} \
    --message-body "$body"
done
```

---

### 4. Aurora CPU High Alarm

**Nome**: `fibonacci-aurora-cpu-high-{env}`

**Descri√ß√£o**: Detecta CPU alta no cluster Aurora

**M√©trica**: RDS CPUUtilization (Average)

**Threshold**: ‚â• 80% por 2 de 3 per√≠odos de 5 minutos

**A√ß√£o Recomendada**:
1. Verificar queries lentas no Aurora:
   ```sql
   SELECT * FROM pg_stat_statements 
   ORDER BY total_time DESC 
   LIMIT 10;
   ```
2. Verificar n√∫mero de conex√µes ativas:
   ```sql
   SELECT count(*) FROM pg_stat_activity;
   ```
3. Considerar aumentar `serverlessV2MaxCapacity` no CDK
4. Implementar connection pooling via RDS Proxy
5. Otimizar queries com √≠ndices apropriados

**Severidade**: üü° ALTA

---

### 5. Estimated Cost High Alarm

**Nome**: `fibonacci-estimated-cost-high-{env}`

**Descri√ß√£o**: Detecta invoca√ß√µes Lambda acima do esperado (proxy para custos)

**M√©trica**: Lambda Invocations (Sum)

**Threshold**: 
- Dev: ‚â• 10,000 invoca√ß√µes/hora
- Staging: ‚â• 10,000 invoca√ß√µes/hora
- Prod: ‚â• 50,000 invoca√ß√µes/hora

**A√ß√£o Recomendada**:
1. Verificar se h√° loop infinito ou retry excessivo
2. Verificar se h√° ataque DDoS (verificar WAF logs)
3. Verificar se h√° campanha de marketing n√£o planejada
4. Revisar AWS Cost Explorer para custos reais
5. Implementar rate limiting mais agressivo se necess√°rio

**Severidade**: üü° M√âDIA

**Nota**: Para monitoramento de custos real, configure AWS Budgets:
```bash
aws budgets create-budget \
  --account-id {account-id} \
  --budget file://budget.json \
  --notifications-with-subscribers file://notifications.json
```

---

### 6. API Gateway 5xx Errors Alarm

**Nome**: `fibonacci-api-5xx-errors-{env}`

**Descri√ß√£o**: Detecta erros 5xx no API Gateway

**M√©trica**: API Gateway 5XXError (Sum)

**Threshold**: ‚â• 5 erros em 2 minutos consecutivos

**A√ß√£o Recomendada**:
1. Verificar logs do API Gateway
2. Verificar se Lambda est√° retornando erros 500
3. Verificar se h√° timeout na Lambda
4. Verificar se h√° problema de permiss√µes IAM
5. Verificar integra√ß√µes downstream

**Severidade**: üî¥ CR√çTICA

---

### 7. Lambda Throttle Alarm

**Nome**: `fibonacci-lambda-throttles-{env}`

**Descri√ß√£o**: Detecta throttling da Lambda (concorr√™ncia excedida)

**M√©trica**: Lambda Throttles (Sum)

**Threshold**: ‚â• 10 throttles em 2 per√≠odos de 5 minutos

**A√ß√£o Recomendada**:
1. Verificar limite de concorr√™ncia da conta AWS:
   ```bash
   aws lambda get-account-settings
   ```
2. Considerar adicionar reserved concurrency:
   ```typescript
   this.apiHandler.addReservedConcurrentExecutions(100);
   ```
3. Implementar backoff exponencial no cliente
4. Solicitar aumento de limite via AWS Support

**Severidade**: üü° ALTA

---

### 8. Old Messages Alarm

**Nome**: `fibonacci-old-messages-{env}`

**Descri√ß√£o**: Detecta mensagens antigas na fila principal

**M√©trica**: SQS ApproximateAgeOfOldestMessage (Maximum)

**Threshold**: ‚â• 300 segundos (5 minutos)

**A√ß√£o Recomendada**:
1. Verificar se consumers est√£o processando mensagens
2. Verificar se h√° erro recorrente impedindo processamento
3. Verificar se h√° backpressure de sistemas downstream
4. Considerar aumentar n√∫mero de consumers (Lambda concurrency)
5. Verificar se visibilityTimeout est√° apropriado

**Severidade**: üü° M√âDIA

---

### 9. Critical System Alarm (Composite)

**Nome**: `fibonacci-critical-system-{env}`

**Descri√ß√£o**: Alarme composto que dispara quando m√∫ltiplos alarmes est√£o ativos

**Regra**: 
- High Error Rate ALARM **OU**
- (High Latency ALARM **E** DLQ Not Empty ALARM)

**A√ß√£o Recomendada**:
1. **ESCALA√á√ÉO IMEDIATA** para equipe de plant√£o
2. Verificar status de todos os componentes
3. Considerar rollback para vers√£o anterior
4. Ativar plano de disaster recovery se necess√°rio
5. Comunicar status aos stakeholders

**Severidade**: üî¥ CR√çTICA - ESCALA√á√ÉO AUTOM√ÅTICA

---

## Configura√ß√£o de Notifica√ß√µes

### Adicionar Email Subscription

```bash
# Via AWS CLI
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:{account}:fibonacci-alarms-{env} \
  --protocol email \
  --notification-endpoint ops@alquimista.ai

# Confirmar subscription no email recebido
```

### Adicionar Slack Webhook (Futuro)

```typescript
// Em lib/fibonacci-stack.ts
import * as chatbot from 'aws-cdk-lib/aws-chatbot';

const slackChannel = new chatbot.SlackChannelConfiguration(this, 'SlackChannel', {
  slackChannelConfigurationName: 'fibonacci-alerts',
  slackWorkspaceId: 'YOUR_WORKSPACE_ID',
  slackChannelId: 'YOUR_CHANNEL_ID'
});

alarmTopic.addSubscription(
  new sns_subscriptions.LambdaSubscription(slackNotifierLambda)
);
```

### Adicionar PagerDuty (Futuro)

```bash
# Criar integration key no PagerDuty
# Adicionar como HTTPS subscription no SNS
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:{account}:fibonacci-alarms-{env} \
  --protocol https \
  --notification-endpoint https://events.pagerduty.com/integration/{key}/enqueue
```

## Dashboards de Alarmes

Os alarmes s√£o visualizados no CloudWatch Dashboard:

1. **Fibonacci Core Dashboard**: M√©tricas principais + status de alarmes
2. **Alarms Dashboard** (criar manualmente):
   - Status de todos os alarmes
   - Hist√≥rico de transi√ß√µes (OK ‚Üí ALARM ‚Üí OK)
   - Tempo m√©dio de resolu√ß√£o (MTTR)

## Testes de Alarmes

### Testar High Error Rate Alarm

```bash
# Gerar erros propositalmente
for i in {1..15}; do
  curl -X POST https://{api-url}/events \
    -H "Content-Type: application/json" \
    -d '{"invalid": "payload"}'
done

# Verificar alarme no console CloudWatch
aws cloudwatch describe-alarms \
  --alarm-names fibonacci-high-error-rate-{env} \
  --query 'MetricAlarms[0].StateValue'
```

### Testar DLQ Alarm

```bash
# Enviar mensagem inv√°lida para fila
aws sqs send-message \
  --queue-url https://sqs.us-east-1.amazonaws.com/{account}/fibonacci-main-{env} \
  --message-body '{"invalid": "message without required fields"}'

# Aguardar 3 tentativas de processamento (falhar√° e ir√° para DLQ)
# Verificar alarme
aws cloudwatch describe-alarms \
  --alarm-names fibonacci-dlq-not-empty-{env}
```

## M√©tricas de SLA

Com base nos alarmes, definimos os seguintes SLAs:

| M√©trica | Target | Alarme |
|---------|--------|--------|
| Availability | 99.9% | High Error Rate |
| Latency (P95) | < 3s | High Latency |
| Error Rate | < 0.1% | High Error Rate |
| Message Processing | < 5min | Old Messages |

## Runbooks

### Runbook: High Error Rate

1. **Identificar**: Verificar logs e X-Ray traces
2. **Isolar**: Identificar se √© problema espec√≠fico de um agente
3. **Mitigar**: 
   - Se problema em agente espec√≠fico: desabilitar agente
   - Se problema geral: rollback para vers√£o anterior
4. **Resolver**: Corrigir c√≥digo e fazer deploy
5. **Validar**: Monitorar m√©tricas por 30 minutos
6. **Documentar**: Adicionar post-mortem

### Runbook: DLQ Not Empty

1. **Inspecionar**: Verificar mensagens na DLQ
2. **Classificar**: Identificar tipo de falha (transient vs permanent)
3. **Corrigir**: 
   - Transient: Reprocessar mensagens
   - Permanent: Corrigir c√≥digo e reprocessar
4. **Validar**: Confirmar que mensagens foram processadas
5. **Limpar**: Purgar DLQ
6. **Prevenir**: Adicionar valida√ß√£o para evitar recorr√™ncia

## Custos de Alarmes

| Recurso | Quantidade | Custo Mensal |
|---------|-----------|--------------|
| CloudWatch Alarms | 9 alarmes | $0.90 |
| SNS Topic | 1 topic | $0.00 |
| SNS Notifications | ~1000/m√™s | $0.50 |
| **Total** | | **~$1.40/m√™s** |

## Pr√≥ximos Passos

1. ‚úÖ Implementar alarmes b√°sicos
2. ‚è≥ Configurar email subscriptions
3. ‚è≥ Integrar com Slack
4. ‚è≥ Integrar com PagerDuty
5. ‚è≥ Criar runbooks automatizados (Lambda)
6. ‚è≥ Implementar auto-remediation para problemas comuns
7. ‚è≥ Criar dashboard de SLA
8. ‚è≥ Implementar anomaly detection com CloudWatch Anomaly Detection

## Refer√™ncias

- [CloudWatch Alarms Documentation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)
- [CloudWatch Composite Alarms](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alarm.html)
- [SNS Best Practices](https://docs.aws.amazon.com/sns/latest/dg/sns-best-practices.html)
- [Lambda Monitoring Best Practices](https://docs.aws.amazon.com/lambda/latest/dg/monitoring-metrics.html)

---

**√öltima atualiza√ß√£o**: 2025-01-12  
**Vers√£o**: 1.0  
**Respons√°vel**: Equipe DevOps Alquimista.AI
